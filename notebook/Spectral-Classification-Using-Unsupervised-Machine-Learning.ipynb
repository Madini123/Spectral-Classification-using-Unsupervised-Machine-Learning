{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib . pyplot as plt\n",
    "from mpl_toolkits . mplot3d import Axes3D\n",
    "from sklearn . decomposition import PCA\n",
    "from sklearn . preprocessing import LabelEncoder , StandardScaler\n",
    "from sklearn . cluster import KMeans , AgglomerativeClustering\n",
    "from sklearn . metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "from astropy .io import fits\n",
    "from scipy . interpolate import interp1d\n",
    "from scipy . ndimage import gaussian_filter1d\n",
    "from scipy . stats import mode\n",
    "import matplotlib .cm as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_spectrum(file_path, cutoff_wavelength=8000):\n",
    "    try:\n",
    "        with fits.open(file_path) as hdul:\n",
    "            if len(hdul) < 3:\n",
    "                raise ValueError(\"Expected at least 3 HDUs in the FITS file\")\n",
    "\n",
    "            data = hdul[1].data\n",
    "            if data is None:\n",
    "                raise ValueError(\"No data found in HDU 1\")\n",
    "\n",
    "            loglam = data['loglam']\n",
    "            flux = data['flux']\n",
    "            wavelength = 10**loglam\n",
    "\n",
    "            mask = wavelength <= cutoff_wavelength\n",
    "            wavelength = wavelength[mask]\n",
    "            flux = flux[mask]\n",
    "\n",
    "            hdu2_data = hdul[2].data\n",
    "            if hdu2_data is None:\n",
    "                raise ValueError(\"No data found in HDU 2\")\n",
    "\n",
    "            class_label = hdu2_data['CLASS'][0].strip()\n",
    "\n",
    "            subclass_label = 'UNKNOWN'\n",
    "            if 'SUBCLASS' in hdu2_data.columns.names:\n",
    "                subclass_value = hdu2_data['SUBCLASS'][0]\n",
    "                if isinstance(subclass_value, str) and subclass_value.strip():\n",
    "                    subclass_label = subclass_value.strip()\n",
    "\n",
    "        return wavelength, flux, class_label, subclass_label\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading spectrum from {file_path}: {e}\")\n",
    "        return None, None, None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalization, smoothing\n",
    "def normalize_flux(flux):\n",
    "    return flux / np.max(flux)\n",
    "\n",
    "def smooth_flux(flux, sigma=2):\n",
    "    return gaussian_filter1d(flux, sigma=sigma)\n",
    "\n",
    "def pre_process_spectrum(wavelength, flux):\n",
    "    flux = normalize_flux(flux)\n",
    "    flux = smooth_flux(flux)\n",
    "    return wavelength, flux\n",
    "\n",
    "def resample_spectrum(wavelength, flux, common_wavelength):\n",
    "    interp_flux = interp1d(wavelength, flux, kind='linear', bounds_error=False, fill_value=\"extrapolate\")\n",
    "    return interp_flux(common_wavelength) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spectra_folder = 'SELECTED_SPECTRA'\n",
    "spectra_files = os.listdir(spectra_folder)\n",
    "\n",
    "all_fluxes = []\n",
    "all_combined_labels = []\n",
    "\n",
    "# Define a common wavelength grid\n",
    "common_wavelength = np.linspace(4000, 8000, 1000)\n",
    "\n",
    "# Load selected spectra\n",
    "for file_name in spectra_files:\n",
    "    file_path = os.path.join(spectra_folder, file_name)\n",
    "    wavelength, flux, class_label, subclass_label = load_spectrum(file_path)\n",
    "    \n",
    "    if wavelength is None or flux is None:\n",
    "        continue\n",
    "    \n",
    "    # Preprocess and resample spectrum\n",
    "    wavelength, flux = pre_process_spectrum(wavelength, flux)\n",
    "    flux = resample_spectrum(wavelength, flux, common_wavelength)\n",
    "    \n",
    "    all_fluxes.append(flux)\n",
    "    all_combined_labels.append(f\"{class_label}_{subclass_label}\")\n",
    "\n",
    "# Stack all fluxes for PCA\n",
    "all_fluxes = np.array(all_fluxes)\n",
    "all_combined_labels = np.array(all_combined_labels)\n",
    "\n",
    "print(f\"Number of spectra after loading: {len(all_fluxes)}\")\n",
    "\n",
    "# Encode the labels to numeric values\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(all_combined_labels)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaled_fluxes = scaler.fit_transform(all_fluxes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform PCA\n",
    "pca = PCA(n_components=3)\n",
    "pca_components = pca.fit_transform(scaled_fluxes) \n",
    "# Perform PCA Plotting in 3D\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Use a larger colormap with many distinct colors\n",
    "cmap = cm.get_cmap('tab20', num_clusters)\n",
    "\n",
    "# Plot the PCA components\n",
    "scatter = ax.scatter(pca_components[:, 0], pca_components[:, 1], pca_components[:, 2], \n",
    "                     c=encoded_labels, cmap=cmap, s=5)\n",
    "\n",
    "# Set axis limits to zoom in on the dense region\n",
    "ax.set_xlim(-40, 55)\n",
    "ax.set_ylim(-40, 20)\n",
    "ax.set_zlim(-30, 50)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_zlabel('PCA Component 3')\n",
    "\n",
    "plt.title('3D PCA Plot of Spectra')\n",
    "\n",
    "# Adjust the viewing angle to better see the layers\n",
    "ax.view_init(elev=20, azim=60)\n",
    "\n",
    "# Directly use the encoded labels for the legend\n",
    "unique_labels = np.unique(encoded_labels)\n",
    "handles = [plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=cmap(i), markersize=5) for i in unique_labels]\n",
    "legend_labels = label_encoder.inverse_transform(unique_labels)\n",
    "\n",
    "# Place the legend outside the plot\n",
    "legend = ax.legend(handles, legend_labels, loc=\"center left\", bbox_to_anchor=(1.05, 0.5), fontsize='small', title=\"Classes\", title_fontsize='medium')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters = len(np.unique(all_combined_labels))\n",
    "\n",
    "# Explained Variance Plot\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(np.arange(1, len(pca.explained_variance_ratio_) + 1), np.cumsum(pca.explained_variance_ratio_), marker='o')\n",
    "plt.xlabel('Number of PCA Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('PCA Explained Variance')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define range of K values to test\n",
    "K = range(1, 55)  # Test cluster sizes from 1 to 14\n",
    "\n",
    "# Initialize lists to store the metrics\n",
    "inertia = []\n",
    "silhouette_scores = []\n",
    "calinski_harabasz_scores = []\n",
    "davies_bouldin_scores = []\n",
    "\n",
    "# Perform K-Means clustering for each K and calculate the metrics\n",
    "for k in K:\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(pca_components)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    inertia.append(kmeans.inertia_)\n",
    "    \n",
    "    if k > 1:  # Silhouette score is undefined for k=1\n",
    "        silhouette_scores.append(silhouette_score(pca_components, cluster_labels))\n",
    "        calinski_harabasz_scores.append(calinski_harabasz_score(pca_components, cluster_labels))\n",
    "        davies_bouldin_scores.append(davies_bouldin_score(pca_components, cluster_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Elbow Method (Inertia)\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 2, 1)\n",
    "plt.plot(K, inertia, 'bo-')\n",
    "plt.title('Elbow Method For Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Inertia')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Silhouette Scores\n",
    "plt.subplot(2, 2, 2)\n",
    "plt.plot(K[1:], silhouette_scores, 'bo-')\n",
    "plt.title('Silhouette Score For Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Silhouette Score')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Calinski-Harabasz Index\n",
    "plt.subplot(2, 2, 3)\n",
    "plt.plot(K[1:], calinski_harabasz_scores, 'bo-')\n",
    "plt.title('Calinski-Harabasz Index For Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Calinski-Harabasz Index')\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Davies-Bouldin Index\n",
    "plt.subplot(2, 2, 4)\n",
    "plt.plot(K[1:], davies_bouldin_scores, 'bo-')\n",
    "plt.title('Davies-Bouldin Index For Optimal K')\n",
    "plt.xlabel('Number of Clusters (K)')\n",
    "plt.ylabel('Davies-Bouldin Index')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to match clusters to original labels\n",
    "def match_labels_to_clusters(original_labels, clusters):\n",
    "    matched_labels = np.zeros_like(clusters)\n",
    "    for cluster in np.unique(clusters):\n",
    "        mask = clusters == cluster\n",
    "        matched_labels[mask] = mode(original_labels[mask])[0]\n",
    "    return matched_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.cm as cm  # Ensure you have imported the colormap module\n",
    "from scipy.stats import mode\n",
    "# Perform K-Means Clustering with the chosen number of clusters\n",
    "optimal_k = 22  \n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "kmeans_clusters = kmeans.fit_predict(pca_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map each cluster to the most frequent original label\n",
    "cluster_labels = {}\n",
    "for cluster in np.unique(kmeans_clusters):\n",
    "    mask = kmeans_clusters == cluster\n",
    "    most_frequent_label = mode(encoded_labels[mask])[0][0]\n",
    "    cluster_labels[cluster] = most_frequent_label\n",
    "\n",
    "# 3D PCA Plot with Centroids\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the PCA components colored by their cluster assignment\n",
    "scatter = ax.scatter(pca_components[:, 0], pca_components[:, 1], pca_components[:, 2],\n",
    "                     c=kmeans_clusters, cmap='tab10', s=5)\n",
    "\n",
    "# Plot the centroids\n",
    "centroids = kmeans.cluster_centers_\n",
    "ax.scatter(centroids[:, 0], centroids[:, 1], centroids[:, 2], c='black', s=100, marker='x', label='Centroids')\n",
    "\n",
    "# Set axis limits to zoom in on the dense region\n",
    "ax.set_xlim(-40, 55)\n",
    "ax.set_ylim(-40, 20)\n",
    "ax.set_zlim(-30, 50)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_zlabel('PCA Component 3')\n",
    "ax.set_title(f'3D PCA Plot with K-Means Clustering (K={optimal_k})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique labels for each cluster in the legend\n",
    "handles = []\n",
    "legend_labels = []\n",
    "for cluster in np.unique(kmeans_clusters):\n",
    "    cluster_color = cm.tab10(cluster % 10)  # Use the colormap from matplotlib.cm\n",
    "    handles.append(plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=cluster_color, markersize=5))\n",
    "    legend_labels.append(f\"Cluster {cluster + 1}: {label_encoder.inverse_transform([cluster_labels[cluster]])[0]}\")\n",
    "\n",
    "# Place the legend outside the plot\n",
    "legend = ax.legend(handles, legend_labels, title=\"K-Means Clusters\", loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "\n",
    "ax.view_init(elev=20, azim=60)\n",
    "\n",
    "plt.show()\n",
    "# 2D PCA Projection: PCA Component 1 vs Component 2\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(pca_components[:, 0], pca_components[:, 1], c=kmeans_clusters, cmap='tab10', s=10, alpha=0.7)\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('2D Projection: PCA Component 1 vs 2')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# 2D PCA Projection: PCA Component 1 vs Component 3\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(pca_components[:, 0], pca_components[:, 2], c=kmeans_clusters, cmap='tab10', s=10, alpha=0.7)\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 3')\n",
    "ax.set_title('2D Projection: PCA Component 1 vs 3')\n",
    "\n",
    "# Show the plot\n",
    "plt.show() \n",
    "# 2D PCA Projection: PCA Component 1 vs Component 3\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(pca_components[:, 1], pca_components[:, 2], c=kmeans_clusters, cmap='tab10', s=10, alpha=0.7)\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('PCA Component 2')\n",
    "ax.set_ylabel('PCA Component 3')\n",
    "ax.set_title('2D Projection: PCA Component 2 vs 3')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Create a DataFrame for analysis\n",
    "cluster_analysis_df = pd.DataFrame({'True_Label': encoded_labels, 'Predicted_Cluster': kmeans_clusters})\n",
    "\n",
    "# Group by predicted clusters and analyze the distribution of true labels\n",
    "for cluster_id in range(optimal_k):\n",
    "    cluster_data = cluster_analysis_df[cluster_analysis_df['Predicted_Cluster'] == cluster_id]\n",
    "    most_common_label, count = mode(cluster_data['True_Label'])\n",
    "    print(f\"Cluster {cluster_id}: Most common label is {most_common_label[0]} with {count[0]} occurrences.\")\n",
    "    print(f\"Label distribution in this cluster:\\n{cluster_data['True_Label'].value_counts(normalize=True)}\\n\")\n",
    "    # Distribution of cluster sizes for K-Means\n",
    "kmeans_cluster_sizes = np.bincount(kmeans_clusters)\n",
    "\n",
    "# Plotting the distribution\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(range(len(kmeans_cluster_sizes)), kmeans_cluster_sizes, color='blue')\n",
    "plt.title('Distribution of Cluster Sizes for K-Means Clustering')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "# Statistical quantities\n",
    "kmeans_silhouette = silhouette_score(pca_components, kmeans_clusters)\n",
    "kmeans_davies_bouldin = davies_bouldin_score(pca_components, kmeans_clusters)\n",
    "kmeans_calinski_harabasz = calinski_harabasz_score(pca_components, kmeans_clusters)\n",
    "kmeans_ari = adjusted_rand_score(encoded_labels, kmeans_clusters)\n",
    "\n",
    "# Calculate the Normalized Mutual Information (NMI)\n",
    "kmeans_nmi = normalized_mutual_info_score(encoded_labels, kmeans_clusters)\n",
    "# Cluster purity\n",
    "def cluster_purity(true_labels, cluster_labels):\n",
    "    contingency_matrix = confusion_matrix(true_labels, cluster_labels)\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "\n",
    "kmeans_purity = cluster_purity(encoded_labels, kmeans_clusters)\n",
    "# Print statistical quantities\n",
    "print(\"K-Means Clustering Statistics:\")\n",
    "print(f\"Silhouette Score: {kmeans_silhouette}\")\n",
    "print(f\"Davies-Bouldin Index: {kmeans_davies_bouldin}\")\n",
    "print(f\"Calinski-Harabasz Index: {kmeans_calinski_harabasz}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {kmeans_ari}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {kmeans_nmi}\")\n",
    "print(f\"Cluster Purity: {kmeans_purity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.cluster.hierarchy import linkage, dendrogram, fcluster\n",
    "\n",
    "# Generate the linkage matrix using 'ward' method (commonly used in hierarchical clustering)\n",
    "Z = linkage(pca_components, method='ward')\n",
    "\n",
    "# Plot the dendrogram\n",
    "plt.figure(figsize=(14, 10))\n",
    "dendro = dendrogram(Z, truncate_mode='level', p=6)  # p=6 means show only the last 6 levels\n",
    "plt.title('Hierarchical Clustering Dendrogram')\n",
    "plt.xlabel('Sample Index')\n",
    "plt.ylabel('Distance')\n",
    "plt.show()\n",
    "from scipy.stats import mode\n",
    "import matplotlib.cm as cm\n",
    "# cmap = cm.get_cmap('tab20', num_clusters)\n",
    "cutoff_distance = 240\n",
    "\n",
    "# Form clusters by cutting the dendrogram at the chosen distance\n",
    "agg_clusters_from_dendrogram = fcluster(Z, cutoff_distance, criterion='distance')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of clusters formed\n",
    "num_clusters_formed = len(np.unique(agg_clusters_from_dendrogram))\n",
    "print(f'Number of clusters formed: {num_clusters_formed}')\n",
    "cmap = cm.get_cmap('tab20', num_clusters_formed)\n",
    "# Assuming you have the `agg_cluster_labels` that map clusters to CLASS_SUBCLASS names\n",
    "unique_labels = np.unique(agg_cluster_labels)\n",
    "num_labels = len(unique_labels)\n",
    "\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the PCA components colored by their cluster assignment from the dendrogram\n",
    "scatter = ax.scatter(pca_components[:, 0], pca_components[:, 1], pca_components[:, 2],\n",
    "                     c=agg_clusters_from_dendrogram, cmap='tab10', s=5)\n",
    "\n",
    "# Set axis limits to zoom in on the dense region\n",
    "ax.set_xlim(-40, 55)\n",
    "ax.set_ylim(-40, 20)\n",
    "ax.set_zlim(-30, 50)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_zlabel('PCA Component 3')\n",
    "plt.title(f'3D PCA Plot with Agglomerative Clustering (Cutoff at {cutoff_distance})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to map each cluster to the most frequent CLASS_SUBCLASS label\n",
    "cluster_to_class_subclass = {}\n",
    "\n",
    "# For each cluster, find the most frequent CLASS_SUBCLASS label\n",
    "for cluster in np.unique(agg_clusters_from_dendrogram):\n",
    "    mask = agg_clusters_from_dendrogram == cluster  # Select data points in the current cluster\n",
    "    most_frequent_label = mode(encoded_labels[mask])[0][0]  # Find the most frequent label in this cluster\n",
    "    cluster_to_class_subclass[cluster] = most_frequent_label\n",
    "\n",
    "# Decode the labels to get the actual CLASS_SUBCLASS names\n",
    "cluster_to_class_subclass_names = {cluster: label_encoder.inverse_transform([label])[0] \n",
    "                                   for cluster, label in cluster_to_class_subclass.items()}\n",
    "agg_cluster_labels = np.array([\n",
    "    cluster_to_class_subclass_names[cluster] \n",
    "    for cluster in agg_clusters_from_dendrogram\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print out the mapping\n",
    "for cluster, class_subclass in cluster_to_class_subclass_names.items():\n",
    "  print(f\"Cluster {cluster}: {class_subclass}\")\n",
    "\n",
    "agg_cluster_labels = np.array([cluster_to_class_subclass_names[cluster] \n",
    "                               for cluster in agg_clusters_from_dendrogram])\n",
    "ax.view_init(elev=20, azim=60)\n",
    "\n",
    "plt.show()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the legend using cluster numbers and CLASS_SUBCLASS names\n",
    "handles = []\n",
    "legend_labels = []\n",
    "for cluster in np.unique(agg_clusters_from_dendrogram):\n",
    "    cluster_color = plt.cm.tab20(cluster % 20)  # Use tab20 colormap with cycling colors\n",
    "    handles.append(plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=cluster_color, markersize=5))\n",
    "    legend_labels.append(f\"Cluster {cluster}: {cluster_to_class_subclass_names[cluster]}\")\n",
    "\n",
    "# Place the legend outside the plot\n",
    "legend = ax.legend(handles, legend_labels, title=\"Agglomerative Clusters\", loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "\n",
    "# Adjust the viewing angle to better see the clusters\n",
    "ax.view_init(elev=20, azim=60)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mode\n",
    "import matplotlib.cm as cm\n",
    "# Perform Agglomerative Clustering with the chosen number of clusters\n",
    "optimal_k = 22 \n",
    "agg_clustering = AgglomerativeClustering(n_clusters=optimal_k)\n",
    "agg_clusters = agg_clustering.fit_predict(pca_components)\n",
    "\n",
    "# Create a dictionary to map each cluster to the most frequent original label\n",
    "cluster_labels = {}\n",
    "for cluster in np.unique(agg_clusters):\n",
    "    mask = agg_clusters == cluster\n",
    "    most_frequent_label = mode(encoded_labels[mask])[0][0]\n",
    "    cluster_labels[cluster] = most_frequent_label\n",
    "\n",
    "# 3D PCA Plot with Centroids\n",
    "fig = plt.figure(figsize=(14, 10))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Plot the PCA components colored by their cluster assignment\n",
    "scatter = ax.scatter(pca_components[:, 0], pca_components[:, 1], pca_components[:, 2],\n",
    "                     c=agg_clusters, cmap='tab10', s=5)\n",
    "\n",
    "# Set axis limits to zoom in on the dense region\n",
    "ax.set_xlim(-40, 55)\n",
    "ax.set_ylim(-40, 20)\n",
    "ax.set_zlim(-30, 50)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_zlabel('PCA Component 3')\n",
    "ax.set_title(f'3D PCA Plot with Agglomerative Clustering (K={optimal_k})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate unique labels for each cluster in the legend\n",
    "handles = []\n",
    "legend_labels = []\n",
    "for cluster in np.unique(agg_clusters):\n",
    "    cluster_color = cm.tab10(cluster % 10)  # Use the colormap from matplotlib.cm\n",
    "    handles.append(plt.Line2D([0], [0], marker='o', color='w', markerfacecolor=cluster_color, markersize=5))\n",
    "    legend_labels.append(f\"Cluster {cluster + 1}: {label_encoder.inverse_transform([cluster_labels[cluster]])[0]}\")\n",
    "\n",
    "# Place the legend outside the plot\n",
    "legend = ax.legend(handles, legend_labels, title=\"Agglomerative Clustering Clusters\", loc=\"center left\", bbox_to_anchor=(1.05, 0.5))\n",
    "\n",
    "ax.view_init(elev=20, azim=60)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# 2D PCA Projection: PCA Component 1 vs Component 2\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(pca_components[:, 0], pca_components[:, 1], c=agg_clusters, cmap='tab10', s=10, alpha=0.7)\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 2')\n",
    "ax.set_title('2D Projection: PCA Component 1 vs 2')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# 2D PCA Projection: PCA Component 1 vs Component 3\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(pca_components[:, 0], pca_components[:, 2], c=agg_clusters, cmap='tab10', s=10, alpha=0.7)\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('PCA Component 1')\n",
    "ax.set_ylabel('PCA Component 3')\n",
    "ax.set_title('2D Projection: PCA Component 1 vs 3')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "# 2D PCA Projection: PCA Component 1 vs Component 3\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(pca_components[:, 1], pca_components[:, 2], c=agg_clusters, cmap='tab10', s=10, alpha=0.7)\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('PCA Component 2')\n",
    "ax.set_ylabel('PCA Component 3')\n",
    "ax.set_title('2D Projection: PCA Component 2 vs 3')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
    "\n",
    "agg_silhouette = silhouette_score(pca_components, agg_clusters)\n",
    "agg_davies_bouldin = davies_bouldin_score(pca_components, agg_clusters)\n",
    "agg_calinski_harabasz = calinski_harabasz_score(pca_components, agg_clusters)\n",
    "agg_ari = adjusted_rand_score(encoded_labels, agg_clusters)\n",
    "agg_nmi = normalized_mutual_info_score(encoded_labels, agg_clusters)\n",
    "# Cluster purity\n",
    "def cluster_purity(true_labels, cluster_labels):\n",
    "    contingency_matrix = confusion_matrix(true_labels, cluster_labels)\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)\n",
    "agg_purity = cluster_purity(encoded_labels, agg_clusters)\n",
    "print(\"\\nAgglomerative Clustering Statistics:\")\n",
    "print(f\"Silhouette Score: {agg_silhouette}\")\n",
    "print(f\"Davies-Bouldin Index: {agg_davies_bouldin}\")\n",
    "print(f\"Calinski-Harabasz Index: {agg_calinski_harabasz}\")\n",
    "print(f\"Adjusted Rand Index (ARI): {agg_ari}\")\n",
    "print(f\"Normalized Mutual Information (NMI): {agg_nmi}\")\n",
    "print(f\"Cluster Purity: {agg_purity}\")\n",
    "# Distribution of cluster sizes for Agglomerative Clustering\n",
    "agg_cluster_sizes = np.bincount(agg_clusters)\n",
    "plt.figure(figsize=(10, 7))\n",
    "plt.bar(range(len(agg_cluster_sizes)), agg_cluster_sizes, color='green')\n",
    "plt.title('Distribution of Cluster Sizes for Agglomerative Clustering')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Size')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "original_class_names = label_encoder.inverse_transform(encoded_labels)\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "cluster_analysis_df = pd.DataFrame({\n",
    "    'True_Label': encoded_labels,            # Encoded labels\n",
    "    'True_Class_Name': original_class_names, # Decoded class names\n",
    "    'Predicted_Cluster': agg_clusters        # Clusters from agglomerative clustering\n",
    "})\n",
    "\n",
    "# Group by predicted clusters and analyze the distribution of true labels\n",
    "for cluster_id in range(optimal_k):  \n",
    "    cluster_data = cluster_analysis_df[cluster_analysis_df['Predicted_Cluster'] == cluster_id]\n",
    "    most_common_label_encoded, count = mode(cluster_data['True_Label'])\n",
    "    most_common_class_name = label_encoder.inverse_transform([most_common_label_encoded[0]])[0]\n",
    "    \n",
    "    print(f\"Cluster {cluster_id}: Most common class is '{most_common_class_name}' with {count[0]} occurrences.\")\n",
    "    print(f\"Class distribution in this cluster:\\n{cluster_data['True_Class_Name'].value_counts(normalize=True)}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
